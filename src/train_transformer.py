# core/train_transformer.py

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import load_model, save_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from src.transformer_model import build_transformer_model
import logging

# Configuration du logging
logger = logging.getLogger('TransformerTraining')
logger.setLevel(logging.INFO)

# Configuration des r√©pertoires
MODEL_DIR = "models/transformer"
REPORT_DIR = "reports/transformer_predictions"
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def train_transformer_models(dataset: dict, lookback: int = 60):
    """
    Entra√Æne les mod√®les Transformer pour tous les symboles du dataset
    
    Args:
        dataset: Dictionnaire {symbol: DataFrame} des donn√©es pr√©par√©es
        lookback: Nombre de p√©riodes historiques √† utiliser
    """
    if not dataset:
        logger.error("‚ùå Aucune donn√©e disponible pour l'entra√Ænement Transformer")
        return
    
    logger.info(f"üß† D√©but de l'entra√Ænement des mod√®les Transformer sur {len(dataset)} symboles")
    
    for symbol, df in dataset.items():
        try:
            logger.info(f"üîß Pr√©paration des donn√©es pour: {symbol}")
            
            # V√©rifier les donn√©es minimales
            if len(df) < lookback * 2:
                logger.warning(f"‚ö†Ô∏è Donn√©es insuffisantes pour {symbol} ({len(df)} < {lookback*2})")
                continue
            
            # Pr√©parer les caract√©ristiques et la cible
            features = df.drop(columns=["timestamp", "symbol", "target"], errors="ignore").values
            target = df["target"].values
            
            # Cr√©er des s√©quences temporelles
            X, y = [], []
            for i in range(lookback, len(features)):
                X.append(features[i-lookback:i])
                y.append(target[i])
            
            X, y = np.array(X), np.array(y)
            
            # V√©rifier les dimensions
            if len(X) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune s√©quence cr√©√©e pour {symbol}")
                continue
                
            logger.info(f"üì¶ Donn√©es pr√©par√©es: {X.shape} s√©quences pour {symbol}")
            
            # Diviser en train/validation
            split_idx = int(len(X) * 0.8)
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            # Construire le mod√®le
            input_shape = (lookback, X_train.shape[2])
            model = build_transformer_model(
                sequence_len=lookback,
                num_features=X_train.shape[2]
            )
            
            # Callbacks
            early_stop = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )
            
            checkpoint = ModelCheckpoint(
                os.path.join(MODEL_DIR, f"{symbol.replace('/', '_')}_best.h5"),
                save_best_only=True,
                monitor='val_loss',
                mode='min'
            )
            
            # Entra√Ænement
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=100,
                batch_size=64,
                callbacks=[early_stop, checkpoint],
                verbose=1
            )
            
            # Sauvegarder le mod√®le final
            model_path = os.path.join(MODEL_DIR, f"{symbol.replace('/', '_')}_transformer.keras")
            save_model(model, model_path)
            logger.info(f"‚úÖ Mod√®le sauvegard√©: {model_path}")
            
            # √âvaluation et visualisation
            evaluate_transformer_model(symbol, X_val, y_val)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur entra√Ænement {symbol}: {str(e)}")

def evaluate_transformer_model(symbol: str, X_val: np.ndarray, y_val: np.ndarray):
    """√âvalue un mod√®le et g√©n√®re un graphique de pr√©diction"""
    try:
        model_path = os.path.join(MODEL_DIR, f"{symbol.replace('/', '_')}_transformer.keras")
        if not os.path.exists(model_path):
            logger.warning(f"‚ö†Ô∏è Mod√®le non trouv√© pour {symbol}: {model_path}")
            return
            
        model = load_model(model_path)
        
        # Pr√©diction
        y_pred = model.predict(X_val, verbose=0).flatten()
        
        # Tracer les r√©sultats
        plt.figure(figsize=(12, 6))
        plt.plot(y_val, label='Valeurs r√©elles', alpha=0.7)
        plt.plot(y_pred, label='Pr√©dictions', alpha=0.7)
        plt.title(f"Pr√©dictions vs R√©el - {symbol}")
        plt.xlabel('P√©riode')
        plt.ylabel('Valeur')
        plt.legend()
        plt.grid(True)
        
        # Sauvegarder le graphique
        plot_path = os.path.join(REPORT_DIR, f"{symbol.replace('/', '_')}_predictions.png")
        plt.savefig(plot_path)
        plt.close()
        logger.info(f"üìä Graphique sauvegard√©: {plot_path}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur √©valuation {symbol}: {str(e)}")

def load_transformer_model(symbol: str):
    """Charge un mod√®le Transformer sauvegard√©"""
    try:
        model_path = os.path.join(MODEL_DIR, f"{symbol.replace('/', '_')}_transformer.keras")
        if not os.path.exists(model_path):
            logger.warning(f"‚ö†Ô∏è Mod√®le non trouv√© pour {symbol}: {model_path}")
            return None
            
        return load_model(model_path)
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement mod√®le {symbol}: {str(e)}")
        return None
